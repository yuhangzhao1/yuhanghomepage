<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | Yuhang Zhao </title> <meta name="author" content="Yuhang Zhao"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?64a8851ed36aad74b39738c8658b6a71"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuhangzhao1.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuhang </span> Zhao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">MadAbility Lab </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GazePrompt-16-9-480.webp 480w,/assets/img/publication_preview/GazePrompt-16-9-800.webp 800w,/assets/img/publication_preview/GazePrompt-16-9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/GazePrompt-16-9.gif" class="preview" width="150" height="130" alt="GazePrompt-16-9.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024gazeprompt" class="col-sm-8"> <div class="title">GazePrompt: Enhancing Low Vision People’s Reading Experience with Gaze-Aware Augmentations</div> <div class="title"><em>CHI 2024</em></div> <div class="author"> Ru Wang ,  Zach Potter ,  Yun Ho ,  Daniel Killough ,  Linda Zeng ,  Sanbrita Mondal ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Reading is a challenging task for low vision people. While conventional low vision aids (e.g., magnification) offer certain support, they cannot fully address the difficulties faced by low vision users, such as locating the next line and distinguishing similar words. To fill this gap, we present <b>GazePrompt</b>, a gaze-aware reading aid that provides timely and targeted visual and audio augmentations based on users’ gaze behaviors. GazePrompt includes two key features: (1) a Line-Switching support that highlights the line a reader intends to read; and (2) a Difficult-Word support that magnifies or reads aloud a word that the reader hesitates with. Through a study with 13 low vision participants who performed well-controlled reading-aloud tasks with and without GazePrompt, we found that GazePrompt significantly reduced participants’ line switching time, reduced word recognition errors, and improved their subjective reading experiences. A follow-up silent-reading study showed that GazePrompt can enhance users’ concentration and perceived comprehension of the reading contents. We further derive design considerations for future gaze-based low vision aids.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/NIST-480.webp 480w,/assets/img/publication_preview/NIST-800.webp 800w,/assets/img/publication_preview/NIST-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/NIST.png" class="preview" width="150" height="130" alt="NIST.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2024exploring" class="col-sm-8"> <div class="title">Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field</div> <div class="title"><em>CHI 2024</em></div> <div class="author"> Kexin Zhang ,  Brianna R Cochran ,  Ruijia Chen ,  Lance Hargung ,  Bryce Sprecher ,  Ross Tredinnick ,  Kevin Ponto ,  Suman Banerjee ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>First responders navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. %First responders perform dangerous and time sensitive tasks in \changemass-casualty incidents hazardous, unfamiliar environments, making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design spaces have not been thoroughly explored by involving various first responders. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed different first responders’ unique preferences and needs for AR cues and interactions, and identified desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/voicechanger-480.webp 480w,/assets/img/publication_preview/voicechanger-800.webp 800w,/assets/img/publication_preview/voicechanger-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/voicechanger.png" class="preview" width="150" height="130" alt="voicechanger.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="povinelli2024springboard" class="col-sm-8"> <div class="title">Springboard, Roadblock or "Crutch"?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality</div> <div class="title"><em>IEEE VR 2024</em></div> <div class="author"> Kassie Povinelli ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Social virtual reality (VR) serves as a vital platform for transgender individuals to explore their identities through avatars and foster personal connections within online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. Prior research acknowledges this issue but overlooks the potential solution of voice changers. We interviewed 13 transgender and gender-nonconforming users of social VR platforms, focusing on their experiences with and without voice changers. We found that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through both hearing their modified voice and the reactions of others to their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified the technical barriers to current voice changer technology and potential improvements to alleviate the problems that transgender and gender-nonconforming users face.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatar-480.webp 480w,/assets/img/publication_preview/avatar-800.webp 800w,/assets/img/publication_preview/avatar-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/avatar.png" class="preview" width="150" height="130" alt="avatar.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023diary" class="col-sm-8"> <div class="title">A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities</div> <div class="title"><em>ASSETS 2023</em></div> <div class="author"> Kexin Zhang ,  Elmira Deldari ,  Yaxing Yao ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388?casa_token=jLUHVfut7IgAAAAA:nenvMgIl7_F5YiBLHVdg_wicjNnATnoz7MdWa5_RDGI_ClXjFBCXuUTC0xh6zmDdUNYz2L8Sq4UuLA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instructions-480.webp 480w,/assets/img/publication_preview/instructions-800.webp 800w,/assets/img/publication_preview/instructions-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/instructions.png" class="preview" width="150" height="130" alt="instructions.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023practices" class="col-sm-8"> <div class="title">Practices and Barriers of Cooking Training for Blind and Low Vision People</div> <div class="title"><em>ASSETS 2023</em></div> <div class="author"> Ru Wang ,  Nihan Zhou ,  Tam Nguyen ,  Sanbrita Mondal ,  Bilge Mutlu ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2310.05396" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3614494" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Cooking is a vital yet challenging activity for blind and low vision (BLV) people, which involves many visual tasks that can be difficult and dangerous. BLV training services, such as vision rehabilitation, can effectively improve BLV people’s independence and quality of life in daily tasks, such as cooking. However, there is a lack of understanding on the practices employed by the training professionals and the barriers faced by BLV people in such training. To fill the gap, we interviewed six professionals to explore their training strategies and technology recommendations for BLV clients in cooking activities. Our findings revealed the fundamental principles, practices, and barriers in current BLV training services, identifying the gaps between training and reality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gazeoverlay-480.webp 480w,/assets/img/publication_preview/gazeoverlay-800.webp 800w,/assets/img/publication_preview/gazeoverlay-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/gazeoverlay.jpeg" class="preview" width="150" height="130" alt="gazeoverlay.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023understanding" class="col-sm-8"> <div class="title">Understanding How Low Vision People Read Using Eye Tracking</div> <div class="title"><em>CHI 2023</em></div> <div class="author"> Ru Wang ,  Linxiu Zeng ,  Xinyong Zhang ,  Sanbrita Mondal ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581213?casa_token=65SwRttGZ3YAAAAA:Y8ZG_9uzrSQTQNfwjg_oKgokmo_stRcHJVRN7_i56WFLWSumf29oeRGBO8i2_lg9voe7BLkOxb4KpEc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While being able to read with screen magnifiers, low vision people have slow and unpleasant reading experiences.Eye tracking has the potential to improve their experience by recognizing fine-grained gaze behaviors and providing more targeted enhancements. To inspire gaze-based low vision technology, <b>we investigate the suitable method to collect low vision users’ gaze data via commercial eye trackers and thoroughly explore their challenges in reading based on their gaze behaviors</b>. With an improved calibration interface, we collected the gaze data of 20 low vision participants and 20 sighted controls who performed reading tasks on a computer screen; low vision participants were also asked to read with different screen magnifiers. We found that, with an accessible calibration interface and data collection method, commercial eye trackers can collect gaze data of comparable quality from low vision and sighted people. Our study identified low vision people’s unique gaze patterns during reading, building upon which, we propose design implications for gaze-based low vision technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 abbr"> </div> <div id="zhao2023if" class="col-sm-8"> <div class="title">{“If} sighted people know, I should be able to {know:”} Privacy Perceptions of Bystanders with Visual Impairments around Camera-based Technology</div> <div class="title"><em>USENIX Security 2023</em></div> <div class="author"> <em>Yuhang Zhao</em> ,  Yaxing Yao ,  Jiaru Fu ,  and  Nihan Zhou </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 abbr"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/izmKY17CDhg?si=1mqWSMUC_qnAMU0x" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""> </iframe> </div> <div id="zhao2019seeingvr" class="col-sm-8"> <div class="title">SeeingVR: A set of tools to make virtual reality more accessible to people with low vision</div> <div class="title"><em>CHI 2019</em></div> <div class="author"> <em>Yuhang Zhao</em> ,  Edward Cutrell ,  Christian Holz ,  Meredith Ringel Morris ,  Eyal Ofek ,  and  Andrew D Wilson </div> <div class="links"> <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300341?casa_token=F95sK5_FZU0AAAAA:G04PVw98T40fHibgDJfk6bOv67nPxXsuQEc6-vxWNa3GjpHuX_-_oMyj7c7tfVfyeo0YCg8ClIyKDQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/microsoft/SeeingVRtoolkit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yuhang Zhao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>